{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analyses\n",
    "Here, we'll perform various analysis by constructing graphs and measure properties of those graphs to learn more about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from scipy.spatial import Delaunay\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = open('../data/data.csv', 'r').readlines()\n",
    "fieldnames = ['x', 'y', 'z', 'unmasked', 'synapses']\n",
    "reader = csv.reader(data)\n",
    "reader.next()\n",
    "\n",
    "rows = [[int(col) for col in row] for row in reader]\n",
    "\n",
    "# These will come in handy later\n",
    "sorted_x = sorted(list(set([r[0] for r in rows])))\n",
    "sorted_y = sorted(list(set([r[1] for r in rows])))\n",
    "sorted_z = sorted(list(set([r[2] for r in rows])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with just looking at analysis in euclidian space, then thinking about weighing by synaptic density later. Since we hypothesize that our data will show that tissue varies as we move down the y-axis (z-axis in brain) through cortical layers, an interesting thing to do would be compare properties of the graphs on each layer (ie how does graph connectivity vary as we move through layers).\n",
    "\n",
    "Let's start by triangulating our data. We'll use Delaunay on each y layer first. Putting our data in the right format for doing graph analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array(rows)\n",
    "b = np.delete(a, np.s_[3::],1)\n",
    "\n",
    "# Separate layers - have to do some wonky stuff to get this to work\n",
    "b = sorted(b, key=lambda e: e[1])\n",
    "b = np.array([v.tolist() for v in b])\n",
    "b = np.split(b, np.where(np.diff(b[:,1]))[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that our data is in the right format, we'll create 52 delaunay graphs. Then we'll perform analyses on these graphs. A simple but useful metric would be to analyze edge length distributions in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graphs = []\n",
    "centroid_list = []\n",
    "\n",
    "for layer in b:\n",
    "    centroids = np.array(layer)\n",
    "    \n",
    "    # get rid of the y value - not relevant anymore\n",
    "    centroids = np.delete(centroids, 1, 1)\n",
    "    centroid_list.append(centroids)\n",
    "    \n",
    "    graph = Delaunay(centroids)\n",
    "    graphs.append(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need a method to get edge lengths from 2D centroid pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d_edge_length(edge):\n",
    "    (x1, y1), (x2, y2) = edge\n",
    "    return math.sqrt((x2-x1)**2 + (y2-y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edge_length_list = [[]]\n",
    "tri_area_list = [[]]\n",
    "\n",
    "for del_graph in graphs:\n",
    "    \n",
    "    tri_areas = []\n",
    "    edge_lengths = []\n",
    "    triangles = []\n",
    "\n",
    "    for t in centroids[del_graph.simplices]:\n",
    "        triangles.append(t)\n",
    "        a, b, c = [tuple(map(int,list(v))) for v in t]\n",
    "        edge_lengths.append(get_d_edge_length((a,b)))\n",
    "        edge_lengths.append(get_d_edge_length((a,c)))\n",
    "        edge_lengths.append(get_d_edge_length((b,c)))\n",
    "        try:\n",
    "            tri_areas.append(float(Triangle(a,b,c).area))\n",
    "        except:\n",
    "            continue\n",
    "    edge_length_list.append(edge_lengths)\n",
    "    tri_area_list.append(tri_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizing after all this that simply location is useless. We know the voxels are evenly spaced, which means our edge length data will be all the same. See that the \"centroids\" are no different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       ..., \n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.subtract(centroid_list[0], centroid_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no distance between the two. Therefore it is perhaps more useful to consider a graph that considers node weights. Voronoi is dual to Delaunay, so that's not much of an option. We want something that considers both spacial location and density similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# edge_length_list[3]\n",
    "# tri_area_list[3]\n",
    "# triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note for future\n",
    "# del_features['d_edge_length_mean'] = np.mean(edge_lengths)\n",
    "# del_features['d_edge_length_std'] = np.std(edge_lengths)\n",
    "# del_features['d_edge_length_skew'] = scipy.stats.skew(edge_lengths)\n",
    "# del_features['d_edge_length_kurtosis'] = scipy.stats.kurtosis(edge_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
