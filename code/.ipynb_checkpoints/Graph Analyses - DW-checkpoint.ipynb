{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analyses\n",
    "Here, we'll perform various analysis by constructing graphs and measure properties of those graphs to learn more about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from scipy.spatial import Delaunay\n",
    "import numpy as np\n",
    "import math\n",
    "from skimage import future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = open('../data/data.csv', 'r').readlines()\n",
    "fieldnames = ['x', 'y', 'z', 'unmasked', 'synapses']\n",
    "reader = csv.reader(data)\n",
    "reader.next()\n",
    "\n",
    "rows = [[int(col) for col in row] for row in reader]\n",
    "\n",
    "# These will come in handy later\n",
    "sorted_x = sorted(list(set([r[0] for r in rows])))\n",
    "sorted_y = sorted(list(set([r[1] for r in rows])))\n",
    "sorted_z = sorted(list(set([r[2] for r in rows])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with just looking at analysis in euclidian space, then thinking about weighing by synaptic density later. Since we hypothesize that our data will show that tissue varies as we move down the y-axis (z-axis in brain) through cortical layers, an interesting thing to do would be compare properties of the graphs on each layer (ie how does graph connectivity vary as we move through layers).\n",
    "\n",
    "Let's start by triangulating our data. We'll use Delaunay on each y layer first. Putting our data in the right format for doing graph analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array(rows)\n",
    "b = np.delete(a, np.s_[3::],1)\n",
    "\n",
    "# Separate layers - have to do some wonky stuff to get this to work\n",
    "b = sorted(b, key=lambda e: e[1])\n",
    "b = np.array([v.tolist() for v in b])\n",
    "b = np.split(b, np.where(np.diff(b[:,1]))[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that our data is in the right format, we'll create 52 delaunay graphs. Then we'll perform analyses on these graphs. A simple but useful metric would be to analyze edge length distributions in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graphs = []\n",
    "centroid_list = []\n",
    "\n",
    "for layer in b:\n",
    "    centroids = np.array(layer)\n",
    "    \n",
    "    # get rid of the y value - not relevant anymore\n",
    "    centroids = np.delete(centroids, 1, 1)\n",
    "    centroid_list.append(centroids)\n",
    "    \n",
    "    graph = Delaunay(centroids)\n",
    "    graphs.append(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need a method to get edge lengths from 2D centroid pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d_edge_length(edge):\n",
    "    (x1, y1), (x2, y2) = edge\n",
    "    return math.sqrt((x2-x1)**2 + (y2-y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edge_length_list = [[]]\n",
    "tri_area_list = [[]]\n",
    "\n",
    "for del_graph in graphs:\n",
    "    \n",
    "    tri_areas = []\n",
    "    edge_lengths = []\n",
    "    triangles = []\n",
    "\n",
    "    for t in centroids[del_graph.simplices]:\n",
    "        triangles.append(t)\n",
    "        a, b, c = [tuple(map(int,list(v))) for v in t]\n",
    "        edge_lengths.append(get_d_edge_length((a,b)))\n",
    "        edge_lengths.append(get_d_edge_length((a,c)))\n",
    "        edge_lengths.append(get_d_edge_length((b,c)))\n",
    "        try:\n",
    "            tri_areas.append(float(Triangle(a,b,c).area))\n",
    "        except:\n",
    "            continue\n",
    "    edge_length_list.append(edge_lengths)\n",
    "    tri_area_list.append(tri_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizing after all this that simply location is useless. We know the voxels are evenly spaced, which means our edge length data will be all the same. See that the \"centroids\" are no different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       ..., \n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.subtract(centroid_list[0], centroid_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no distance between the two. Therefore it is perhaps more useful to consider a graph that considers node weights. Voronoi is dual to Delaunay, so that's not much of an option. We want something that considers both spacial location and density similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Region Adjacency Graph (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past we've considered density information alone (ie analysis density histogram distribution) and above we are considering only spacial information, which totally doesn't say anything. To construct a graph that consider both spacial and density information, we'll use a Region Adjacency Graph (RAG).\n",
    "\n",
    "In RAGs, two nodes are considered as neighbor if they are close in proximity (separated by a small number of pixels/voxels) in the horizontal or vertical direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../docs/figures/plot_rag_1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data includes density values at each node (voxel, or pixel since we're looking at y-layers), we can weight by the inverse of density difference between two nodes. Inverse because strongly connected nodes should be close in weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have number of synapses S<sub>i</sub> at nodes $i$ and define weights $w$ between the nodes:\n",
    "\n",
    "$$w = \\dfrac{1}{S_i - S_{i+1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGs are used largely in image processing, and it makes sense for our data to look more like an image. Since the data is evenly spaced, the absolute locations of the voxels don't matter. We can use the index in the matrix to represent spacial location, with the value at each pixel being the synapse density at that voxel. We've done this before in \"real volume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_volume = np.zeros((len(sorted_x), len(sorted_y), len(sorted_z)))\n",
    "for r in rows:\n",
    "    real_volume[sorted_x.index(r[0]), sorted_y.index(r[1]), sorted_z.index(r[2])] = r[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_layers = np.split(real_volume, np.where(np.diff(real_volume[:,1]))[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# real_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# edge_length_list[3]\n",
    "# tri_area_list[3]\n",
    "# triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note for future\n",
    "# del_features['d_edge_length_mean'] = np.mean(edge_lengths)\n",
    "# del_features['d_edge_length_std'] = np.std(edge_lengths)\n",
    "# del_features['d_edge_length_skew'] = scipy.stats.skew(edge_lengths)\n",
    "# del_features['d_edge_length_kurtosis'] = scipy.stats.kurtosis(edge_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
